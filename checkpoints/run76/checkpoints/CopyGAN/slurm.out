starting training run 76
----------------- Options ---------------
              D_headstart: 1000                          	[default: 0]
              D_threshold: 0.5                           
       accumulation_steps: 1                             
               batch_size: 64                            
                    beta1: 0.5                           
                    beta2: 0.999                         
          checkpoints_dir: /scratch/checkpoints          	[default: ./checkpoints]
        confidence_weight: 0.0                           
           continue_train: False                         
                crop_size: 64                            
                 dataroot: /scratch/datasets/CLEVR_colorized/images	[default: datasets]
             dataset_mode: double                        
                direction: None                          
              display_env: main                          
             display_freq: 100                           
               display_id: 0                             	[default: 1]
            display_ncols: 4                             
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
            flip_vertical: False                         
                 gan_mode: vanilla                       
                  gpu_ids: 0                             
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
          keep_last_batch: False                         
               lambda_aux: 1.0                           	[default: 0.1]
                load_iter: 0                             	[default: 0]
                load_size: 70                            
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: step                          
         max_dataset_size: inf                           
          min_obj_surface: 100                           
                    model: copy                          
                 n_epochs: 5                             	[default: 20]
           n_epochs_decay: 0                             	[default: 10]
               n_layers_D: 3                             
                     name: CopyGAN                       
                      ndf: 64                            
                     netD: copy                          
                     netG: copy                          
                      ngf: 64                            
             no_alternate: False                         
        no_border_zeroing: False                         
               no_dropout: False                         
                  no_flip: False                         
               no_grfakes: False                         
                  no_html: False                         
                     norm: instance                      
              num_threads: 4                             
                output_nc: 3                             
                    phase: train                         
                   pool_D: True                          	[default: False]
                pool_size: 50                            
               preprocess: resize_and_crop               
               print_freq: 100                           	[default: 20]
              real_target: 0.75                          	[default: 0.8]
             save_by_iter: False                         
          save_epoch_freq: 10                            
         save_latest_freq: 5000                          
                     seed: 42                            	[default: 0]
           serial_batches: False                         
               sigma_blur: 1.0                           
                   suffix:                               
              tracemalloc: False                         
         update_html_freq: 100                           
           val_batch_size: 128                           
                 val_freq: 100                           
                  verbose: True                          	[default: False]
----------------- End -------------------
dataset [DoubleDataset] and dataloder are created
dataset [DoubleDataset] and dataloder are created
Starting training of copy-model
The number of training images = 26000
The number of validation images = 3000
The number of epochs to run = 5
gpu_ids: [0]
initialize network with normal
gpu_ids: [0]
initialize network with normal
model [CopyModel] was created
---------- Networks initialized -------------
DataParallel(
  (module): CopyGenerator(
    (enc1): EncoderBlock(
      (model): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (2): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (enc2): EncoderBlock(
      (model): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=replicate)
        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (2): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (enc3): EncoderBlock(
      (model): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=replicate)
        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (2): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (enc4): EncoderBlock(
      (model): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=replicate)
        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (2): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (dec4): DecoderBlock(
      (model): Sequential(
        (0): Upsample(scale_factor=2.0, mode=bilinear)
        (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (3): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (dec3): DecoderBlock(
      (model): Sequential(
        (0): Upsample(scale_factor=2.0, mode=bilinear)
        (1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (3): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (dec2): DecoderBlock(
      (model): Sequential(
        (0): Upsample(scale_factor=2.0, mode=bilinear)
        (1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
        (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (3): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (dec1): DecoderBlock(
      (model): Sequential(
        (0): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
      )
    )
    (sigmoid): Sigmoid()
  )
)
[Network G] Total number of parameters : 3.469 M
DataParallel(
  (module): CopyDiscriminator(
    (blur_filter): GaussianSmoothing()
    (enc1): EncoderBlock(
      (model): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (2): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (enc2): EncoderBlock(
      (model): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=replicate)
        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (2): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (enc3): EncoderBlock(
      (model): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=replicate)
        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (2): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (enc4): EncoderBlock(
      (model): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=replicate)
        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (2): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (dec4): DecoderBlock(
      (model): Sequential(
        (0): Upsample(scale_factor=2.0, mode=bilinear)
        (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (3): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (dec3): DecoderBlock(
      (model): Sequential(
        (0): Upsample(scale_factor=2.0, mode=bilinear)
        (1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (3): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (dec2): DecoderBlock(
      (model): Sequential(
        (0): Upsample(scale_factor=2.0, mode=bilinear)
        (1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
        (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (3): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (dec1): DecoderBlock(
      (model): Sequential(
        (0): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
      )
    )
    (sigmoid): Sigmoid()
    (pred_layers): Sequential(
      (0): AdaptiveAvgPool2d(output_size=1)
      (1): Flatten(start_dim=1, end_dim=-1)
      (2): Linear(in_features=512, out_features=256, bias=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Sigmoid()
    )
  )
)
[Network D] Total number of parameters : 3.600 M
-----------------------------------------------
create web directory /scratch/checkpoints/CopyGAN/web...
validation accuracies:
                gf: 0.00
                real: 1.00
                fake: 0.00

ran validation set (B:1) in                         34.6 s.
(epoch: 1, batches: 100, time: 0.018, data: 0.003) loss_G_comp: 0.000 loss_G_anti_sc: 0.000 loss_G: 0.000 loss_D_real: 0.968 loss_D_fake: 0.160 loss_D: 3.282 acc_real: 1.000 acc_fake: 0.000 loss_AUX: 1.947 loss_D_gr_fake: 0.207 acc_grfake: 0.000 
validation accuracies:
                gf: 1.00
                real: 0.00
                fake: 1.00

ran validation set (B:101) in                         33.1 s.
(epoch: 1, batches: 200, time: 0.018, data: 0.003) loss_G_comp: 0.000 loss_G_anti_sc: 0.000 loss_G: 0.000 loss_D_real: 0.718 loss_D_fake: 0.032 loss_D: 2.547 acc_real: 0.000 acc_fake: 1.000 loss_AUX: 1.694 loss_D_gr_fake: 0.103 acc_grfake: 1.000 
validation accuracies:
                gf: 0.96
                real: 0.68
                fake: 0.99

ran validation set (B:201) in                         33.3 s.
(epoch: 1, batches: 300, time: 0.018, data: 0.003) loss_G_comp: 0.000 loss_G_anti_sc: 0.000 loss_G: 0.000 loss_D_real: 0.678 loss_D_fake: 0.014 loss_D: 2.453 acc_real: 0.681 acc_fake: 0.992 loss_AUX: 1.681 loss_D_gr_fake: 0.080 acc_grfake: 0.958 
validation accuracies:
                gf: 0.93
                real: 0.92
                fake: 0.98

ran validation set (B:301) in                         33.8 s.
(epoch: 1, batches: 400, time: 0.018, data: 0.003) loss_G_comp: 0.000 loss_G_anti_sc: 0.000 loss_G: 0.000 loss_D_real: 0.608 loss_D_fake: 0.018 loss_D: 2.312 acc_real: 0.921 acc_fake: 0.980 loss_AUX: 1.622 loss_D_gr_fake: 0.064 acc_grfake: 0.934 
validation accuracies:
                gf: 0.96
                real: 0.86
                fake: 0.99

ran validation set (B:401) in                         33.5 s.
learning rate 0.0002000 -> 0.0002000
End of epoch 1 / 5 	 Time Taken: 430 sec
/home/tlotze/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
validation accuracies:
                gf: 0.94
                real: 0.91
                fake: 0.98

ran validation set (B:501) in                         33.8 s.
(epoch: 2, batches: 100, time: 0.017, data: 0.003) loss_G_comp: 0.000 loss_G_anti_sc: 0.000 loss_G: 0.000 loss_D_real: 0.624 loss_D_fake: 0.026 loss_D: 2.345 acc_real: 0.910 acc_fake: 0.979 loss_AUX: 1.599 loss_D_gr_fake: 0.096 acc_grfake: 0.941 
validation accuracies:
                gf: 0.93
                real: 0.94
                fake: 0.98

ran validation set (B:601) in                         33.6 s.
(epoch: 2, batches: 200, time: 0.017, data: 0.003) loss_G_comp: 0.000 loss_G_anti_sc: 0.000 loss_G: 0.000 loss_D_real: 0.617 loss_D_fake: 0.007 loss_D: 2.240 acc_real: 0.941 acc_fake: 0.982 loss_AUX: 1.571 loss_D_gr_fake: 0.045 acc_grfake: 0.929 
validation accuracies:
                gf: 0.93
                real: 0.96
                fake: 0.98

ran validation set (B:701) in                         34.5 s.
(epoch: 2, batches: 300, time: 0.017, data: 0.003) loss_G_comp: 0.000 loss_G_anti_sc: 0.000 loss_G: 0.000 loss_D_real: 0.649 loss_D_fake: 0.015 loss_D: 2.279 acc_real: 0.963 acc_fake: 0.981 loss_AUX: 1.556 loss_D_gr_fake: 0.059 acc_grfake: 0.927 
validation accuracies:
                gf: 0.97
                real: 0.83
                fake: 1.00

ran validation set (B:801) in                         33.4 s.
(epoch: 2, batches: 400, time: 0.017, data: 0.003) loss_G_comp: 0.000 loss_G_anti_sc: 0.000 loss_G: 0.000 loss_D_real: 0.582 loss_D_fake: 0.013 loss_D: 2.155 acc_real: 0.834 acc_fake: 0.999 loss_AUX: 1.519 loss_D_gr_fake: 0.041 acc_grfake: 0.966 
learning rate 0.0002000 -> 0.0002000
End of epoch 2 / 5 	 Time Taken: 397 sec
validation accuracies:
                gf: 0.95
                real: 0.92
                fake: 1.00

ran validation set (B:901) in                         33.4 s.
(epoch: 3, batches: 100, time: 0.017, data: 0.003) loss_G_comp: 0.000 loss_G_anti_sc: 0.000 loss_G: 0.000 loss_D_real: 0.575 loss_D_fake: 0.020 loss_D: 2.214 acc_real: 0.916 acc_fake: 0.996 loss_AUX: 1.545 loss_D_gr_fake: 0.074 acc_grfake: 0.954 
Headstart D over
validation accuracies:
                gf: 0.95
                real: 0.95
                fake: 0.99

ran validation set (B:1001) in                         34.2 s.
(epoch: 3, batches: 200, time: 0.012, data: 0.003) loss_G_comp: 1.853 loss_G_anti_sc: 0.158 loss_G: 2.011 loss_D_real: 0.984 loss_D_fake: 0.036 loss_D: 2.951 acc_real: 0.950 acc_fake: 0.994 loss_AUX: 1.920 loss_D_gr_fake: 0.011 acc_grfake: 0.954 
validation accuracies:
                gf: 0.99
                real: 0.40
                fake: 0.98

ran validation set (B:1101) in                         33.7 s.
(epoch: 3, batches: 300, time: 0.012, data: 0.003) loss_G_comp: 1.780 loss_G_anti_sc: 0.172 loss_G: 1.953 loss_D_real: 0.807 loss_D_fake: 0.330 loss_D: 2.031 acc_real: 0.398 acc_fake: 0.980 loss_AUX: 0.811 loss_D_gr_fake: 0.083 acc_grfake: 0.990 
validation accuracies:
                gf: 0.99
                real: 0.45
                fake: 0.83

ran validation set (B:1201) in                         33.3 s.
(epoch: 3, batches: 400, time: 0.012, data: 0.003) loss_G_comp: 0.837 loss_G_anti_sc: 0.509 loss_G: 1.346 loss_D_real: 0.912 loss_D_fake: 0.382 loss_D: 1.791 acc_real: 0.448 acc_fake: 0.834 loss_AUX: 0.470 loss_D_gr_fake: 0.027 acc_grfake: 0.987 
learning rate 0.0002000 -> 0.0002000
End of epoch 3 / 5 	 Time Taken: 368 sec
validation accuracies:
                gf: 1.00
                real: 0.01
                fake: 0.99

ran validation set (B:1301) in                         33.5 s.
(epoch: 4, batches: 100, time: 0.013, data: 0.003) loss_G_comp: 0.989 loss_G_anti_sc: 0.370 loss_G: 1.359 loss_D_real: 0.898 loss_D_fake: 0.412 loss_D: 1.766 acc_real: 0.011 acc_fake: 0.995 loss_AUX: 0.411 loss_D_gr_fake: 0.045 acc_grfake: 1.000 
validation accuracies:
                gf: 1.00
                real: 0.06
                fake: 0.97

ran validation set (B:1401) in                         33.1 s.
(epoch: 4, batches: 200, time: 0.013, data: 0.003) loss_G_comp: 0.971 loss_G_anti_sc: 0.385 loss_G: 1.357 loss_D_real: 0.923 loss_D_fake: 0.390 loss_D: 1.708 acc_real: 0.056 acc_fake: 0.975 loss_AUX: 0.354 loss_D_gr_fake: 0.041 acc_grfake: 1.000 
validation accuracies:
                gf: 1.00
                real: 0.01
                fake: 1.00

ran validation set (B:1501) in                         33.3 s.
(epoch: 4, batches: 300, time: 0.012, data: 0.003) loss_G_comp: 1.002 loss_G_anti_sc: 0.405 loss_G: 1.407 loss_D_real: 0.781 loss_D_fake: 0.546 loss_D: 1.723 acc_real: 0.006 acc_fake: 0.997 loss_AUX: 0.340 loss_D_gr_fake: 0.056 acc_grfake: 1.000 
validation accuracies:
                gf: 1.00
                real: 0.11
                fake: 0.93

ran validation set (B:1601) in                         33.3 s.
(epoch: 4, batches: 400, time: 0.012, data: 0.003) loss_G_comp: 0.896 loss_G_anti_sc: 0.436 loss_G: 1.332 loss_D_real: 0.965 loss_D_fake: 0.383 loss_D: 1.697 acc_real: 0.110 acc_fake: 0.931 loss_AUX: 0.325 loss_D_gr_fake: 0.024 acc_grfake: 0.997 
learning rate 0.0002000 -> 0.0002000
End of epoch 4 / 5 	 Time Taken: 342 sec
validation accuracies:
                gf: 1.00
                real: 0.07
                fake: 0.97

ran validation set (B:1701) in                         34.2 s.
(epoch: 5, batches: 100, time: 0.013, data: 0.003) loss_G_comp: 0.798 loss_G_anti_sc: 0.539 loss_G: 1.337 loss_D_real: 0.992 loss_D_fake: 0.339 loss_D: 1.649 acc_real: 0.066 acc_fake: 0.968 loss_AUX: 0.295 loss_D_gr_fake: 0.022 acc_grfake: 1.000 
validation accuracies:
                gf: 1.00
                real: 0.00
                fake: 1.00

ran validation set (B:1801) in                         33.5 s.
(epoch: 5, batches: 200, time: 0.012, data: 0.003) loss_G_comp: 0.971 loss_G_anti_sc: 0.389 loss_G: 1.360 loss_D_real: 0.794 loss_D_fake: 0.521 loss_D: 1.650 acc_real: 0.002 acc_fake: 0.999 loss_AUX: 0.301 loss_D_gr_fake: 0.034 acc_grfake: 1.000 
validation accuracies:
                gf: 1.00
                real: 0.00
                fake: 1.00

ran validation set (B:1901) in                         34.0 s.
(epoch: 5, batches: 300, time: 0.012, data: 0.003) loss_G_comp: 0.872 loss_G_anti_sc: 0.457 loss_G: 1.329 loss_D_real: 0.867 loss_D_fake: 0.443 loss_D: 1.626 acc_real: 0.000 acc_fake: 1.000 loss_AUX: 0.289 loss_D_gr_fake: 0.026 acc_grfake: 1.000 
validation accuracies:
                gf: 1.00
                real: 0.00
                fake: 1.00

ran validation set (B:2001) in                         32.8 s.
(epoch: 5, batches: 400, time: 0.012, data: 0.003) loss_G_comp: 0.889 loss_G_anti_sc: 0.449 loss_G: 1.338 loss_D_real: 0.870 loss_D_fake: 0.444 loss_D: 1.605 acc_real: 0.000 acc_fake: 1.000 loss_AUX: 0.279 loss_D_gr_fake: 0.013 acc_grfake: 1.000 
learning rate 0.0002000 -> 0.0001600
End of epoch 5 / 5 	 Time Taken: 343 sec
Finished training, model is saved
Batches trained - G: 515, D: 1515 
